import requests
from bs4 import BeautifulSoup
import os
import shutil
import time
import certifi
from tqdm import tqdm
import pandas as pd
import json
from datetime import datetime

import urllib3
# å¿½ç•¥ SSL è­‰æ›¸è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

"""
TWSE è²¡å ±è³‡æ–™ä¸‹è¼‰èˆ‡åˆä½µå·¥å…·
==============================

æ–°å¢åŠŸèƒ½èªªæ˜ï¼š
1. åƒ…åˆä½µæ¨¡å¼ï¼šè¨­å®š only_merge = Trueï¼Œè·³éä¸‹è¼‰ç›´æ¥åˆä½µç¾æœ‰çš„ raw_data æª”æ¡ˆ
2. æ¬„ä½éæ¿¾ï¼šè¨­å®š keep_columns å­—å…¸ä¾†æŒ‡å®šæ¯ç¨®å ±è¡¨è¦ä¿ç•™çš„æ¬„ä½
3. è‡ªå‹•æ’åºï¼šåˆä½µå¾Œçš„è³‡æ–™æœƒè‡ªå‹•ä¾å…¬å¸ä»£è™Ÿæ’åº

è™•ç†æµç¨‹ï¼š
1. ä¸‹è¼‰æˆ–è®€å– CSV æª”æ¡ˆ
2. åˆä½µæ‰€æœ‰è³‡æ–™
3. éæ¿¾æŒ‡å®šæ¬„ä½ï¼ˆå¦‚æœ‰è¨­å®šï¼‰
4. ä¾å…¬å¸ä»£è™Ÿæ’åº
5. å„²å­˜ç‚º CSV/JSON æ ¼å¼

ä½¿ç”¨ç¯„ä¾‹ï¼š
---------
# ä¸€èˆ¬æ¨¡å¼ï¼ˆä¸‹è¼‰ + åˆä½µï¼‰
only_merge = False
keep_columns = {}

# åƒ…åˆä½µæ¨¡å¼ + æ¬„ä½éæ¿¾
only_merge = True
keep_columns = {
    'balance_sheet': ['å…¬å¸ä»£è™Ÿ', 'å…¬å¸åç¨±', 'æµå‹•è³‡ç”¢', 'è³‡ç”¢ç¸½é¡'],
    'income_statement': ['å…¬å¸ä»£è™Ÿ', 'å…¬å¸åç¨±', 'ç‡Ÿæ¥­æ”¶å…¥', 'ç¨…å¾Œæ·¨åˆ©'],
    'dividend': ['å…¬å¸ä»£è™Ÿåç¨±', 'è‚¡æ±æœƒæ—¥æœŸ', 'è‚¡åˆ©åˆè¨ˆ']
}
"""

# =========================
# Config
# =========================
start_year = 107
end_year = 114
markets = ["sii", "otc"]
seasons = ["01", "02", "03", "04"]

download_reports = ['all']  # è™•ç†æ‰€æœ‰å ±è¡¨é¡å‹
save_format = ['csv', 'json']  # å¯ç‚º ['csv'], ['json'], ['csv', 'json']

# æ–°å¢åŠŸèƒ½è¨­å®š
only_merge = True  # è¨­ç‚º True æ™‚åªåšåˆä½µï¼Œä¸ä¸‹è¼‰

# æŒ‡å®šè¦ä¿ç•™çš„æ¬„ä½ï¼Œæ ¼å¼: {'report_name': ['column1', 'column2', ...]}
# æ¬„ä½ä¿ç•™è¨­å®šç¯„ä¾‹ï¼š
# keep_columns = {
#     'balance_sheet': ['å…¬å¸ä»£è™Ÿ', 'å…¬å¸åç¨±', 'æµå‹•è³‡ç”¢', 'è³‡ç”¢ç¸½é¡'],
#     'income_statement': ['å…¬å¸ä»£è™Ÿ', 'å…¬å¸åç¨±', 'ç‡Ÿæ¥­æ”¶å…¥', 'ç¨…å¾Œæ·¨åˆ©'],
#     'dividend': ['å…¬å¸ä»£è™Ÿåç¨±', 'è‚¡æ±æœƒæ—¥æœŸ', 'è‚¡åˆ©åˆè¨ˆ'],
#     'cash_flow': ['å…¬å¸ä»£è™Ÿ', 'å…¬å¸åç¨±', 'ç‡Ÿæ¥­æ´»å‹•ä¹‹ç¾é‡‘æµé‡']
# }
keep_columns = {
    'balance_sheet': [
        # è­˜åˆ¥èˆ‡æ™‚é–“åºåˆ— (ä½¿ç”¨è™•ç†å¾Œçš„çµ±ä¸€æ ¼å¼)
        'ä»£è™Ÿ',     # è™•ç†å¾Œï¼šå…¬å¸ä»£è™Ÿ â†’ ä»£è™Ÿ
        'åç¨±',     # è™•ç†å¾Œï¼šå…¬å¸åç¨± â†’ åç¨±
        'å¹´åº¦',     # è™•ç†å¾Œï¼šæ°‘åœ‹å¹´åº¦æ ¼å¼
        'å­£åˆ¥',
        # æ ¸å¿ƒè¨ˆç®— (ROE, ç›ˆå†ç‡) - é€™äº›æ¬„ä½å·²ç¢ºèªå­˜åœ¨æ–¼ Source [1] ä¸­
        'æ­¸å±¬æ–¼æ¯å…¬å¸æ¥­ä¸»ä¹‹æ¬Šç›Šåˆè¨ˆ',  # ROE åˆ†æ¯
        'ä¸å‹•ç”¢åŠè¨­å‚™ï¼æ·¨é¡',       # ç›ˆå†ç‡çµ„ä»¶
        'ç„¡å½¢è³‡ç”¢ï¼æ·¨é¡',           # ç›ˆå†ç‡çµ„ä»¶
        # é¢¨éšªèˆ‡è¼”åŠ©è³‡è¨Š - é€™äº›æ¬„ä½å·²ç¢ºèªå­˜åœ¨æ–¼ Source [1] ä¸­
        'æµå‹•è³‡ç”¢',
        'è³‡ç”¢ç¸½é¡',
        'æµå‹•è² å‚µ',
        'éæµå‹•è² å‚µ',
        'éæ§åˆ¶æ¬Šç›Š',
        'æ¯è‚¡åƒè€ƒæ·¨å€¼',
    ],
    'income_statement': [
        # è­˜åˆ¥èˆ‡æ™‚é–“åºåˆ— (ä½¿ç”¨è™•ç†å¾Œçš„çµ±ä¸€æ ¼å¼)
        'ä»£è™Ÿ',     # è™•ç†å¾Œï¼šå…¬å¸ä»£è™Ÿ â†’ ä»£è™Ÿ
        'åç¨±',     # è™•ç†å¾Œï¼šå…¬å¸åç¨± â†’ åç¨±
        'å¹´åº¦',     # è™•ç†å¾Œï¼šæ°‘åœ‹å¹´åº¦æ ¼å¼
        'å­£åˆ¥',
        'å‡ºè¡¨æ—¥æœŸ',
        # æ ¸å¿ƒè¨ˆç®— (ROE, ç©©å®šæ€§)
        'æ·¨åˆ©ï¼ˆæï¼‰æ­¸å±¬æ–¼æ¯å…¬å¸æ¥­ä¸»',
        'ç‡Ÿæ¥­æ”¶å…¥',
        'ç‡Ÿæ¥­æˆæœ¬',
        # è¼”åŠ©èˆ‡ç›¸å®¹æ€§
        'ç¨…å¾Œæ·¨åˆ©',
        'åŸºæœ¬æ¯è‚¡ç›ˆé¤˜ï¼ˆå…ƒï¼‰',
    ],
    'dividend': [
        # è­˜åˆ¥èˆ‡æ™‚é–“åºåˆ— (ä½¿ç”¨è™•ç†å¾Œçš„çµ±ä¸€æ ¼å¼)
        'ä»£è™Ÿ',        # è™•ç†å¾Œï¼šå…¬å¸ä»£è™Ÿåç¨± â†’ ä»£è™Ÿ
        'åç¨±',        # è™•ç†å¾Œï¼šå…¬å¸ä»£è™Ÿåç¨± â†’ åç¨±
        'å¹´åº¦',        # è™•ç†å¾Œï¼šæ°‘åœ‹å¹´åº¦æ ¼å¼
        'å­£åˆ¥',        # è™•ç†å¾Œï¼šæ¨™æº–åŒ–æ ¼å¼
        'è‚¡æ±æœƒæ—¥æœŸ',
        'è‚¡åˆ©æ‰€å±¬æœŸé–“',
        'æ±ºè­°ï¼ˆæ“¬è­°ï¼‰é€²åº¦',
        # æ ¸å¿ƒè¨ˆç®— (ç¾é‡‘é…ç™¼/IRR)
        'è‚¡æ±é…ç™¼-ç›ˆé¤˜åˆ†é…ä¹‹ç¾é‡‘è‚¡åˆ©(å…ƒ/è‚¡)',
        'è‚¡æ±é…ç™¼-æ³•å®šç›ˆé¤˜å…¬ç©ç™¼æ”¾ä¹‹ç¾é‡‘(å…ƒ/è‚¡)',
        'è‚¡æ±é…ç™¼-è³‡æœ¬å…¬ç©ç™¼æ”¾ä¹‹ç¾é‡‘(å…ƒ/è‚¡)',
        'è‚¡æ±é…ç™¼-è‚¡æ±é…ç™¼ä¹‹ç¾é‡‘(è‚¡åˆ©)ç¸½é‡‘é¡(å…ƒ)',
        # é…è‚¡ç›¸é—œ
        'è‚¡æ±é…ç™¼-ç›ˆé¤˜è½‰å¢è³‡é…è‚¡(å…ƒ/è‚¡)',
        'è‚¡æ±é…ç™¼-æ³•å®šç›ˆé¤˜å…¬ç©è½‰å¢è³‡é…è‚¡(å…ƒ/è‚¡)',
        'è‚¡æ±é…ç™¼-è³‡æœ¬å…¬ç©è½‰å¢è³‡é…è‚¡(å…ƒ/è‚¡)',
        'è‚¡æ±é…ç™¼-è‚¡æ±é…è‚¡ç¸½è‚¡æ•¸(è‚¡)'
    ],
    'cash_flow': [
        # è­˜åˆ¥èˆ‡æ™‚é–“åºåˆ— (ä½¿ç”¨è™•ç†å¾Œçš„çµ±ä¸€æ ¼å¼)
        'ä»£è™Ÿ',     # è™•ç†å¾Œï¼šå…¬å¸ä»£è™Ÿ â†’ ä»£è™Ÿ
        'åç¨±',     # è™•ç†å¾Œï¼šå…¬å¸åç¨± â†’ åç¨±
        'å¹´åº¦',     # è™•ç†å¾Œï¼šæ°‘åœ‹å¹´åº¦æ ¼å¼
        'å­£åˆ¥',
        # æ ¸å¿ƒè¨ˆç®— (é¢¨éšªé©—è­‰)
        'ç‡Ÿæ¥­æ´»å‹•ä¹‹æ·¨ç¾é‡‘æµå…¥ï¼ˆæµå‡ºï¼‰',
    ],
    'etf_dividend': [
        # è­˜åˆ¥èˆ‡æ™‚é–“åºåˆ— (ä½¿ç”¨è™•ç†å¾Œçš„çµ±ä¸€æ ¼å¼)
        'ä»£è™Ÿ',        # è™•ç†å¾Œï¼šè­‰åˆ¸ä»£è™Ÿ â†’ ä»£è™Ÿ
        'åç¨±',        # è™•ç†å¾Œï¼šè­‰åˆ¸ç°¡ç¨± â†’ åç¨±
        'å¹´åº¦',        # è™•ç†å¾Œï¼šæ°‘åœ‹å¹´åº¦æ ¼å¼
        'å­£åˆ¥',        # è™•ç†å¾Œï¼šä¾é™¤æ¯äº¤æ˜“æ—¥åˆ¤æ–·
        # æ—¥æœŸè³‡è¨Š
        'é™¤æ¯äº¤æ˜“æ—¥',
        'æ”¶ç›Šåˆ†é…åŸºæº–æ—¥',
        'æ”¶ç›Šåˆ†é…ç™¼æ”¾æ—¥',
        # æ”¶ç›Šè³‡è¨Š
        'é…æ¯',
        'å…¬å‘Šå¹´åº¦'
    ]
}

report_types = {
    "balance_sheet": {
        "ajax": "https://mopsov.twse.com.tw/mops/web/ajax_t163sb05?year={year}&TYPEK={market}&season={season}&firstin=1",
        "download_base": "https://mopsov.twse.com.tw/server-java/t105sb02"
    },
    "dividend": {
        "ajax": "https://mopsov.twse.com.tw/server-java/t05st09sub?YEAR={year}&qryType=2&TYPEK={market}&step=1",
        "download_base": "https://mopsov.twse.com.tw/server-java/t105sb02"
    },
    "income_statement": {
        "ajax": "https://mopsov.twse.com.tw/mops/web/ajax_t163sb04?year={year}&TYPEK={market}&season={season}&firstin=1",
        "download_base": "https://mopsov.twse.com.tw/server-java/t105sb02"
    },
    "cash_flow": {
        "ajax": "https://mopsov.twse.com.tw/mops/web/ajax_t163sb20?year={year}&TYPEK={market}&season={season}&firstin=1",
        "download_base": "https://mopsov.twse.com.tw/server-java/t105sb02"
    },
    "etf_dividend": {
        "url": "https://www.twse.com.tw/rwd/zh/ETF/etfDiv?stkNo=&startDate={start_date}&endDate={end_date}&response=json",
        "csv_export": "https://www.twse.com.tw/rwd/zh/ETF/etfDiv?stkNo=&startDate={start_date}&endDate={end_date}&response=csv"
    }
}

headers = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)"}

base_dir = "raw_data"
merge_dir = "merged_data"
csv_output_dir = os.path.join(merge_dir, "csv")
json_output_dir = os.path.join(merge_dir, "json")
log_path = os.path.join(merge_dir, "log.json")

# å»ºç«‹è¼¸å‡ºç›®éŒ„
os.makedirs(merge_dir, exist_ok=True)
os.makedirs(csv_output_dir, exist_ok=True)
os.makedirs(json_output_dir, exist_ok=True)

# =========================
# Helper: sort by company code
# =========================
def sort_by_company_code(df: pd.DataFrame, report_name: str) -> pd.DataFrame:
    """ä¾å…¬å¸ä»£è™Ÿæ’åº"""
    if df.empty:
        return df

    # æ‰¾å‡ºä»£è™Ÿæ¬„ä½ (çµ±ä¸€æ ¼å¼)
    company_code_col = None
    if "ä»£è™Ÿ" in df.columns:
        company_code_col = "ä»£è™Ÿ"
    elif "å…¬å¸ä»£è™Ÿ" in df.columns:
        company_code_col = "å…¬å¸ä»£è™Ÿ"
    elif "å…¬å¸ä»£è™Ÿåç¨±" in df.columns:
        company_code_col = "å…¬å¸ä»£è™Ÿåç¨±"
    else:
        # å˜—è©¦æ‰¾åˆ°åŒ…å«"ä»£è™Ÿ"æˆ–"å…¬å¸ä»£è™Ÿ"çš„æ¬„ä½
        for col in df.columns:
            if "ä»£è™Ÿ" in col:
                company_code_col = col
                break

    if company_code_col is None:
        print(f"âš ï¸ {report_name} æ‰¾ä¸åˆ°ä»£è™Ÿæ¬„ä½ï¼Œè·³éæ’åº")
        return df

    print(f"ğŸ”¢ {report_name} ä¾ '{company_code_col}' æ’åº")

    # å¦‚æœæ˜¯å…¬å¸ä»£è™Ÿåç¨±æ ¼å¼ (ä¾‹å¦‚: "2330 - å°ç©é›»")ï¼Œæå–å‰é¢çš„æ•¸å­—é€²è¡Œæ’åº
    if company_code_col == "å…¬å¸ä»£è™Ÿåç¨±" or (len(df) > 0 and " - " in str(df[company_code_col].iloc[0])):
        # å‰µå»ºä¸€å€‹è‡¨æ™‚æ¬„ä½ç”¨æ–¼æ’åº
        df_sorted = df.copy()
        df_sorted['_sort_key'] = df_sorted[company_code_col].astype(str).str.extract(r'(\d+)')[0]
        df_sorted['_sort_key'] = pd.to_numeric(df_sorted['_sort_key'], errors='coerce')
        df_sorted = df_sorted.sort_values(by='_sort_key', ascending=True, ignore_index=True)
        df_sorted = df_sorted.drop(columns=['_sort_key'])
        return df_sorted
    else:
        # ç›´æ¥ä¾å…¬å¸ä»£è™Ÿæ’åºï¼ˆé©ç”¨æ–¼å·²æ‹†åˆ†çš„å…¬å¸ä»£è™Ÿæ¬„ä½ï¼‰
        # ä¿æŒå…¬å¸ä»£è™Ÿç‚ºå­—ä¸²æ ¼å¼ï¼Œä½†ç”¨æ•¸å­—æ’åº
        df_sorted = df.copy()

        # å‰µå»ºè‡¨æ™‚æ’åºéµï¼Œæå–å…¬å¸ä»£è™Ÿä¸­çš„æ•¸å­—éƒ¨åˆ†
        df_sorted['_sort_key'] = df_sorted[company_code_col].astype(str).str.extract(r'(\d+)')[0]
        df_sorted['_sort_key'] = pd.to_numeric(df_sorted['_sort_key'], errors='coerce')

        # æŒ‰æ•¸å­—æ’åºä½†ä¿æŒåŸå§‹å­—ä¸²æ ¼å¼
        df_sorted = df_sorted.sort_values(by='_sort_key', ascending=True, ignore_index=True)
        df_sorted = df_sorted.drop(columns=['_sort_key'])

        return df_sorted


# =========================
# Helper: process company code name
# =========================
def process_company_code_name(df: pd.DataFrame, report_name: str) -> pd.DataFrame:
    """
    è™•ç†å„é¡å ±è¡¨çš„æ¬„ä½æ¨™æº–åŒ–ï¼š
    1. è‚¡åˆ©è³‡æ–™è¡¨ï¼š
       - å…¬å¸ä»£è™Ÿåç¨± â†’ å…¬å¸ä»£è™Ÿ + å…¬å¸åç¨±
       - è‚¡åˆ©æ‰€å±¬å¹´(å­£)åº¦ â†’ å¹´åº¦(æ•´æ•¸) + å­£åˆ¥(Q1, Q2, Q3, Q4, H1, H2, Y1)
    2. å…¶ä»–å ±è¡¨ï¼ˆbalance_sheetã€cash_flowã€income_statementï¼‰ï¼š
       - å­£åˆ¥æ ¼å¼æ¨™æº–åŒ–ï¼š1, 2, 3, 4 â†’ Q1, Q2, Q3, Q4
       - ç¢ºä¿å…¬å¸ä»£è™Ÿç‚ºæ–‡å­—æ ¼å¼

    Args:
        df: è³‡æ–™æ¡†
        report_name: å ±è¡¨åç¨±

    Returns:
        è™•ç†å¾Œçš„è³‡æ–™æ¡†
    """
    if df.empty:
        return df

    df_processed = df.copy()

    # é€šç”¨è™•ç†ï¼šç¢ºä¿å…¬å¸ä»£è™Ÿç‚ºæ–‡å­—æ ¼å¼
    if "å…¬å¸ä»£è™Ÿ" in df_processed.columns:
        df_processed["å…¬å¸ä»£è™Ÿ"] = df_processed["å…¬å¸ä»£è™Ÿ"].astype(str)

    print(f"ğŸ”§ {report_name} æ­£åœ¨è™•ç†æ¬„ä½æ¨™æº–åŒ–...")

    # 1. çµ±ä¸€æ¬„ä½é‡æ–°å‘½å (æ‰€æœ‰å ±è¡¨)
    rename_mapping = {}
    if "å…¬å¸ä»£è™Ÿ" in df_processed.columns:
        rename_mapping["å…¬å¸ä»£è™Ÿ"] = "ä»£è™Ÿ"
    if "å…¬å¸åç¨±" in df_processed.columns:
        rename_mapping["å…¬å¸åç¨±"] = "åç¨±"

    if rename_mapping:
        df_processed = df_processed.rename(columns=rename_mapping)
        print(f"   æ¬„ä½é‡æ–°å‘½å: {rename_mapping}")

    # 2. å¹´åº¦è™•ç†ï¼šä¿æŒæ°‘åœ‹å¹´æ ¼å¼
    if "å¹´åº¦" in df_processed.columns:
        # ç¢ºä¿å¹´åº¦ç‚ºæ•´æ•¸æ ¼å¼ï¼Œä½†ä¿æŒæ°‘åœ‹å¹´
        year_numeric = pd.to_numeric(df_processed["å¹´åº¦"], errors='coerce')
        df_processed["å¹´åº¦"] = year_numeric.astype('Int64')
        print(f"   å¹´åº¦ä¿æŒæ°‘åœ‹å¹´æ ¼å¼")

    # 3. è‚¡åˆ©è³‡æ–™è¡¨å°ˆå±¬è™•ç†
    if report_name == "dividend":

        # æ‹†åˆ†å…¬å¸ä»£è™Ÿåç¨±æ¬„ä½
        if "å…¬å¸ä»£è™Ÿåç¨±" in df_processed.columns:
            print(f"   æ­£åœ¨æ‹†åˆ†å…¬å¸ä»£è™Ÿåç¨±æ¬„ä½...")

            # æ‹†åˆ†å…¬å¸ä»£è™Ÿåç¨± (æ ¼å¼: "1234 - å…¬å¸åç¨±")
            company_info = df_processed["å…¬å¸ä»£è™Ÿåç¨±"].str.split(" - ", n=1, expand=True)

            # æ–°å¢ä»£è™Ÿå’Œåç¨±æ¬„ä½
            df_processed["ä»£è™Ÿ"] = company_info[0].str.strip()
            df_processed["åç¨±"] = company_info[1].str.strip()

            # ç§»é™¤åŸå§‹çš„å…¬å¸ä»£è™Ÿåç¨±æ¬„ä½
            df_processed = df_processed.drop(columns=["å…¬å¸ä»£è™Ÿåç¨±"])

            print(f"   æˆåŠŸæ‹†åˆ†å…¬å¸ä»£è™Ÿåç¨±æ¬„ä½")

        # æ‹†åˆ†è‚¡åˆ©æ‰€å±¬å¹´(å­£)åº¦æ¬„ä½
        if "è‚¡åˆ©æ‰€å±¬å¹´(å­£)åº¦" in df_processed.columns:
            print(f"   æ­£åœ¨æ‹†åˆ†è‚¡åˆ©æ‰€å±¬å¹´(å­£)åº¦æ¬„ä½...")

            # æå–å¹´åº¦ (ä¾‹å¦‚: "113å¹´ å¹´åº¦" â†’ 113)
            year_match = df_processed["è‚¡åˆ©æ‰€å±¬å¹´(å­£)åº¦"].str.extract(r'(\d+)å¹´')[0]
            year_numeric = pd.to_numeric(year_match, errors='coerce').astype('Int64')
            # ä¿æŒæ°‘åœ‹å¹´æ ¼å¼
            df_processed["å¹´åº¦"] = year_numeric

            # æå–å­£åˆ¥ä¸¦æ¨™æº–åŒ–
            def standardize_dividend_period(period_str):
                if pd.isna(period_str):
                    return None

                period_str = str(period_str).strip()

                # å¹´åº¦
                if "å¹´åº¦" in period_str:
                    return "Y1"
                # å­£åº¦
                elif "ç¬¬1å­£" in period_str:
                    return "Q1"
                elif "ç¬¬2å­£" in period_str:
                    return "Q2"
                elif "ç¬¬3å­£" in period_str:
                    return "Q3"
                elif "ç¬¬4å­£" in period_str:
                    return "Q4"
                # åŠå¹´
                elif "ä¸ŠåŠå¹´" in period_str:
                    return "H1"
                elif "ä¸‹åŠå¹´" in period_str:
                    return "H2"
                # æœˆä»½ (å¦‚æœæœ‰çš„è©±)
                elif "æœˆ" in period_str:
                    month_match = pd.Series([period_str]).str.extract(r'ç¬¬?(\d+)æœˆ')[0].iloc[0]
                    if month_match:
                        return f"M{month_match.zfill(2)}"

                return "OTHER"

            df_processed["å­£åˆ¥"] = df_processed["è‚¡åˆ©æ‰€å±¬å¹´(å­£)åº¦"].apply(standardize_dividend_period)

            # ç§»é™¤åŸå§‹çš„è‚¡åˆ©æ‰€å±¬å¹´(å­£)åº¦æ¬„ä½
            df_processed = df_processed.drop(columns=["è‚¡åˆ©æ‰€å±¬å¹´(å­£)åº¦"])

            print(f"   æˆåŠŸæ‹†åˆ†è‚¡åˆ©æ‰€å±¬å¹´(å­£)åº¦æ¬„ä½")

    # 4. å…¶ä»–å ±è¡¨ï¼ˆbalance_sheetã€cash_flowã€income_statementï¼‰è™•ç†
    elif report_name in ["balance_sheet", "cash_flow", "income_statement"]:

        # æ¨™æº–åŒ–å­£åˆ¥æ ¼å¼ï¼š1, 2, 3, 4 â†’ Q1, Q2, Q3, Q4
        if "å­£åˆ¥" in df_processed.columns:
            print(f"   æ­£åœ¨æ¨™æº–åŒ–å­£åˆ¥æ ¼å¼...")

            def standardize_quarter(quarter_val):
                if pd.isna(quarter_val):
                    return None

                quarter_str = str(quarter_val).strip()

                if quarter_str == "1":
                    return "Q1"
                elif quarter_str == "2":
                    return "Q2"
                elif quarter_str == "3":
                    return "Q3"
                elif quarter_str == "4":
                    return "Q4"
                else:
                    return quarter_str  # ä¿æŒåŸå€¼å¦‚æœä¸æ˜¯1-4

            df_processed["å­£åˆ¥"] = df_processed["å­£åˆ¥"].apply(standardize_quarter)

            print(f"   å­£åˆ¥æ¨™æº–åŒ–å®Œæˆï¼š1,2,3,4 â†’ Q1,Q2,Q3,Q4")

    # 5. é‡æ–°æ’åˆ—æ¬„ä½é †åºï¼ˆæ‰€æœ‰å ±è¡¨çµ±ä¸€ï¼‰
    cols = df_processed.columns.tolist()
    priority_cols = []

    for col_name in ['ä»£è™Ÿ', 'åç¨±', 'å¹´åº¦', 'å­£åˆ¥']:
        if col_name in cols:
            priority_cols.append(col_name)
            cols.remove(col_name)

    # é‡æ–°çµ„åˆæ¬„ä½é †åº
    new_cols = priority_cols + cols
    df_processed = df_processed[new_cols]

    # 6. æ•¸å€¼æ¬„ä½è½‰æ›
    numeric_columns = {
        'balance_sheet': [
            'æ­¸å±¬æ–¼æ¯å…¬å¸æ¥­ä¸»ä¹‹æ¬Šç›Šåˆè¨ˆ',
            'ä¸å‹•ç”¢åŠè¨­å‚™ï¼æ·¨é¡',
            'ç„¡å½¢è³‡ç”¢ï¼æ·¨é¡',
            'æµå‹•è³‡ç”¢',
            'è³‡ç”¢ç¸½é¡',
            'æµå‹•è² å‚µ',
            'éæµå‹•è² å‚µ',
            'éæ§åˆ¶æ¬Šç›Š',
            'æ¯è‚¡åƒè€ƒæ·¨å€¼'
        ],
        'income_statement': [
            'æ·¨åˆ©ï¼ˆæï¼‰æ­¸å±¬æ–¼æ¯å…¬å¸æ¥­ä¸»',
            'ç‡Ÿæ¥­æ”¶å…¥',
            'ç‡Ÿæ¥­æˆæœ¬',
            'ç¨…å¾Œæ·¨åˆ©',
            'åŸºæœ¬æ¯è‚¡ç›ˆé¤˜ï¼ˆå…ƒï¼‰'
        ],
        'dividend': [
            'è‚¡æ±é…ç™¼-ç›ˆé¤˜åˆ†é…ä¹‹ç¾é‡‘è‚¡åˆ©(å…ƒ/è‚¡)',
            'è‚¡æ±é…ç™¼-æ³•å®šç›ˆé¤˜å…¬ç©ç™¼æ”¾ä¹‹ç¾é‡‘(å…ƒ/è‚¡)',
            'è‚¡æ±é…ç™¼-è³‡æœ¬å…¬ç©ç™¼æ”¾ä¹‹ç¾é‡‘(å…ƒ/è‚¡)',
            'è‚¡æ±é…ç™¼-è‚¡æ±é…ç™¼ä¹‹ç¾é‡‘(è‚¡åˆ©)ç¸½é‡‘é¡(å…ƒ)',
            'è‚¡æ±é…ç™¼-ç›ˆé¤˜è½‰å¢è³‡é…è‚¡(å…ƒ/è‚¡)',
            'è‚¡æ±é…ç™¼-æ³•å®šç›ˆé¤˜å…¬ç©è½‰å¢è³‡é…è‚¡(å…ƒ/è‚¡)',
            'è‚¡æ±é…ç™¼-è³‡æœ¬å…¬ç©è½‰å¢è³‡é…è‚¡(å…ƒ/è‚¡)',
            'è‚¡æ±é…ç™¼-è‚¡æ±é…è‚¡ç¸½è‚¡æ•¸(è‚¡)'
        ],
        'cash_flow': [
            'ç‡Ÿæ¥­æ´»å‹•ä¹‹æ·¨ç¾é‡‘æµå…¥ï¼ˆæµå‡ºï¼‰'
        ],
        'etf_dividend': [
            'é…æ¯',
            'å…¬å‘Šå¹´åº¦'
        ]
    }

    if report_name in numeric_columns:
        columns_to_convert = numeric_columns[report_name]
        existing_numeric_cols = [col for col in columns_to_convert if col in df_processed.columns]

        if existing_numeric_cols:
            print(f"   è½‰æ›æ•¸å€¼æ¬„ä½: {existing_numeric_cols}")
            for col in existing_numeric_cols:
                # æ¸…ç†æ•¸å€¼ï¼šç§»é™¤é€—è™Ÿã€ç©ºæ ¼ã€ç‰¹æ®Šå­—ç¬¦
                df_processed[col] = df_processed[col].astype(str).str.replace(',', '')
                df_processed[col] = df_processed[col].str.replace(' ', '')
                df_processed[col] = df_processed[col].str.replace('--', '')
                df_processed[col] = df_processed[col].str.replace('-', '')
                df_processed[col] = df_processed[col].replace(['', 'nan', 'None', 'null'], None)

                # è½‰æ›ç‚ºæ•¸å€¼
                df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')

            print(f"   âœ… æˆåŠŸè½‰æ› {len(existing_numeric_cols)} å€‹æ•¸å€¼æ¬„ä½")

    print(f"âœ… {report_name} æ¬„ä½è™•ç†å®Œæˆï¼Œçµ±ä¸€æ ¼å¼ï¼šä»£è™Ÿã€åç¨±ã€å¹´åº¦(æ°‘åœ‹)ã€å­£åˆ¥")

    return df_processed


# =========================
# Helper: filter columns
# =========================
def filter_columns(df: pd.DataFrame, report_name: str) -> pd.DataFrame:
    """æ ¹æ“šè¨­å®šéæ¿¾æ¬„ä½"""
    if not keep_columns or report_name not in keep_columns:
        print(f"ğŸ“‹ {report_name} æœªè¨­å®šæ¬„ä½éæ¿¾ï¼Œä¿ç•™æ‰€æœ‰ {len(df.columns)} æ¬„")
        return df

    columns_to_keep = keep_columns[report_name]
    existing_columns = [col for col in columns_to_keep if col in df.columns]

    if existing_columns:
        missing_columns = set(columns_to_keep) - set(existing_columns)
        if missing_columns:
            print(f"âš ï¸ {report_name} æ‰¾ä¸åˆ°æ¬„ä½: {list(missing_columns)}")

        print(f"ğŸ“‹ {report_name} æ¬„ä½éæ¿¾: {len(df.columns)} â†’ {len(existing_columns)} æ¬„")
        print(f"   ä¿ç•™æ¬„ä½: {existing_columns}")
        return df[existing_columns].copy()
    else:
        print(f"âš ï¸ {report_name} æ‰¾ä¸åˆ°ä»»ä½•æŒ‡å®šçš„æ¬„ä½ï¼Œä¿ç•™æ‰€æœ‰ {len(df.columns)} æ¬„")
        return df


# =========================
# Helper: clean + sort dividend CSV
# =========================
def clean_and_sort_dividend(path: str) -> pd.DataFrame:
    """å¼·åŒ–ç‰ˆè‚¡åˆ©å ±è¡¨æ¸…ç†ï¼šè·³éå‰ç½®èªªæ˜è¡Œï¼Œè¼‰å…¥å¾Œæ’åºä¸¦ç§»é™¤æœ‰å•é¡Œçš„åˆ—"""

    # å…ˆè®€å–æ–‡æœ¬æ‰¾åˆ°çœŸæ­£çš„è¡¨é ­ä½ç½®
    with open(path, "r", encoding="utf-8-sig", errors="ignore") as f:
        lines = f.readlines()

    header_idx = None
    for i, line in enumerate(lines):
        # å°‹æ‰¾åŒ…å« "å…¬å¸ä»£è™Ÿåç¨±" æˆ–åŒæ™‚åŒ…å« "å…¬å¸ä»£è™Ÿ" å’Œ "å…¬å¸åç¨±" çš„è¡¨é ­è¡Œ
        if ("å…¬å¸ä»£è™Ÿåç¨±" in line) or (("å…¬å¸ä»£è™Ÿ" in line) and ("å…¬å¸åç¨±" in line)):
            if line.count(",") > 2:  # ç¢ºä¿æ˜¯è¡¨æ ¼é–‹é ­
                header_idx = i
                break

    if header_idx is None:
        print(f"âš ï¸ ç„¡æ³•åœ¨ {os.path.basename(path)} æ‰¾åˆ°å…¬å¸ä»£è™Ÿæ¬„ä½")
        return pd.DataFrame()

    # ç”¨ pandas è¼‰å…¥ï¼Œè·³éå‰é¢çš„èªªæ˜è¡Œ
    try:
        df = pd.read_csv(path, encoding="utf-8-sig", dtype=str, engine="python",
                        on_bad_lines="skip", skiprows=header_idx)
    except:
        print(f"âš ï¸ ç„¡æ³•è®€å– {os.path.basename(path)}")
        return pd.DataFrame()

    # æª¢æŸ¥æ˜¯å¦æœ‰è³‡æ–™
    if df.empty:
        print(f"âš ï¸ {os.path.basename(path)} ç‚ºç©ºæª”æ¡ˆ")
        return pd.DataFrame()

    # ç¢ºå®šç¬¬ä¸€æ¬„çš„åç¨±ï¼ˆå¯èƒ½æ˜¯ "å…¬å¸ä»£è™Ÿåç¨±" æˆ– "å…¬å¸ä»£è™Ÿ"ï¼‰
    first_col = df.columns[0]
    if "å…¬å¸ä»£è™Ÿ" not in first_col:
        print(f"âš ï¸ ç„¡æ³•åœ¨ {os.path.basename(path)} æ‰¾åˆ°å…¬å¸ä»£è™Ÿæ¬„ä½")
        return pd.DataFrame()

    # ç§»é™¤å…¨ç©ºåˆ—
    df = df.dropna(how="all")

    # ç§»é™¤ Unnamed æ¬„ä½
    df = df.loc[:, ~df.columns.str.contains("^Unnamed")]

    # æŒ‰ç¬¬ä¸€æ¬„æ’åºï¼ˆå°‡æœ‰å•é¡Œçš„åˆ—æ’åˆ°ä¸€èµ·ï¼‰
    df = df.sort_values(by=first_col, ascending=True, ignore_index=True, na_position='last')

    # ç§»é™¤æœ‰å•é¡Œçš„åˆ—ï¼š
    # 1. ç¬¬ä¸€æ¬„ä¸åŒ…å« " - " çš„åˆ—ï¼ˆé™¤äº†è¡¨é ­ï¼‰
    # 2. ç¬¬ä¸€æ¬„åŒ…å«è¡¨é ­æ–‡å­—çš„é‡è¤‡åˆ—
    # 3. ç¬¬ä¸€æ¬„ç‚ºç©ºæˆ–åªæœ‰å°‘é‡æ–‡å­—çš„åˆ—

    mask_to_keep = pd.Series([True] * len(df))

    for i, val in enumerate(df[first_col]):
        val_str = str(val).strip()

        # è·³éç©ºå€¼
        if val_str in ['nan', '', 'None']:
            mask_to_keep[i] = False
            continue

        # ç§»é™¤é‡è¤‡çš„è¡¨é ­è¡Œ
        if "å…¬å¸ä»£è™Ÿ" in val_str and not " - " in val_str:
            mask_to_keep[i] = False
            continue

        # ç§»é™¤ä¸åŒ…å« " - " çš„è¡Œï¼ˆæ­£å¸¸çš„å…¬å¸ä»£è™Ÿæ‡‰è©²æ˜¯ "1234 - å…¬å¸åç¨±" æ ¼å¼ï¼‰
        if " - " not in val_str:
            mask_to_keep[i] = False
            continue

        # ç§»é™¤å¤ªçŸ­çš„è¡Œï¼ˆå¯èƒ½æ˜¯æ–·è¡Œé€ æˆçš„ï¼‰
        if len(val_str) < 5:
            mask_to_keep[i] = False
            continue

    # å¥—ç”¨éæ¿¾
    df_cleaned = df[mask_to_keep].copy()

    # é‡æ–°ç´¢å¼•
    df_cleaned.reset_index(drop=True, inplace=True)

    # æœ€å¾ŒæŒ‰æ­£å¸¸æ¬„ä½é‡æ–°æ’åº
    sort_cols = []
    if "å…¬å¸ä»£è™Ÿ" in df_cleaned.columns:
        sort_cols = [col for col in ["å…¬å¸ä»£è™Ÿ", "å…¬å¸åç¨±", "è‚¡æ±æœƒæ—¥æœŸ"] if col in df_cleaned.columns]
    elif "å…¬å¸ä»£è™Ÿåç¨±" in df_cleaned.columns:
        sort_cols = [col for col in ["å…¬å¸ä»£è™Ÿåç¨±", "è‚¡æ±æœƒæ—¥æœŸ"] if col in df_cleaned.columns]

    if sort_cols:
        df_cleaned = df_cleaned.sort_values(by=sort_cols, ascending=True, ignore_index=True)

    print(f"âœ… {os.path.basename(path)} æ¸…ç†å®Œæˆï¼Œä¿ç•™ {len(df_cleaned)} è¡Œ")

    return df_cleaned



# =========================
# Helper: ETF Dividend Downloader
# =========================
def download_etf_dividend(year_str, year_dir):
    """ä¸‹è¼‰ ETF è‚¡åˆ©è³‡æ–™ - å„ªå…ˆCSVæ ¼å¼"""
    print(f"ğŸ“ˆ ä¸‹è¼‰ {year_str} ETF è‚¡åˆ©è³‡æ–™...")

    # æ°‘åœ‹å¹´è½‰è¥¿å…ƒå¹´
    roc_year = int(year_str)
    ad_year = roc_year + 1911

    # è¨­å®šæ—¥æœŸç¯„åœ (æ•´å¹´åº¦)
    start_date = f"{ad_year}0101"
    end_date = f"{ad_year + 1}0101"  # ä¿®æ­£ï¼šä¸‹ä¸€å¹´çš„1æœˆ1æ—¥

    # ETF è‚¡åˆ© API URL (ä½¿ç”¨æ‚¨æä¾›çš„æ ¼å¼)
    csv_url = f"https://www.twse.com.tw/rwd/zh/ETF/etfDiv?stkNo=&startDate={start_date}&endDate={end_date}&response=csv"
    json_url = f"https://www.twse.com.tw/rwd/zh/ETF/etfDiv?stkNo=&startDate={start_date}&endDate={end_date}&response=json"

    csv_filename = f"etf_dividend_{ad_year}.csv"
    csv_path = os.path.join(year_dir, csv_filename)

    # å„ªå…ˆå˜—è©¦ CSV ä¸‹è¼‰
    print(f"ğŸ”— å„ªå…ˆå˜—è©¦ CSV: {csv_url}")

    try:
        response = requests.get(csv_url, headers=headers, verify=False, timeout=30)
        response.encoding = "utf-8"

        if response.status_code == 200 and len(response.text.strip()) > 100:
            # å„²å­˜ CSV å…§å®¹åˆ° raw_data
            with open(csv_path, 'w', encoding='utf-8-sig', newline='') as f:
                f.write(response.text)

            # æª¢æŸ¥æ˜¯å¦ç‚ºæœ‰æ•ˆçš„CSVæª”æ¡ˆ
            try:
                test_df = pd.read_csv(csv_path, encoding="utf-8-sig", nrows=5)
                if not test_df.empty and len(test_df.columns) > 3:
                    print(f"âœ… ETF è‚¡åˆ© CSV ä¸‹è¼‰æˆåŠŸ: {csv_path}")
                    return True
                else:
                    print(f"âš ï¸ CSV æª”æ¡ˆæ ¼å¼ç•°å¸¸ï¼Œå˜—è©¦ JSON ä¸‹è¼‰")
                    os.remove(csv_path)
            except Exception as e:
                print(f"âš ï¸ CSV æª”æ¡ˆè®€å–å¤±æ•—: {e}ï¼Œå˜—è©¦ JSON ä¸‹è¼‰")
                if os.path.exists(csv_path):
                    os.remove(csv_path)
        else:
            print(f"âš ï¸ CSV å›æ‡‰ç•°å¸¸: status={response.status_code}, length={len(response.text)}")

    except Exception as e:
        print(f"âš ï¸ CSV ä¸‹è¼‰å¤±æ•—: {e}")

    # CSV å¤±æ•—ï¼Œå˜—è©¦ JSON ä¸‹è¼‰ä¸¦è½‰æ›ç‚º CSV
    print(f"ğŸ”„ å˜—è©¦ JSON ä¸‹è¼‰: {json_url}")

    try:
        response = requests.get(json_url, headers=headers, verify=False, timeout=30)
        response.encoding = "utf-8"

        if response.status_code == 200:
            data = response.json()

            # æª¢æŸ¥æ˜¯å¦æœ‰è³‡æ–™
            if 'data' in data and len(data['data']) > 0:
                # è§£æ JSON è³‡æ–™ä¸¦è½‰ç‚º DataFrame
                fields = data.get('fields', [])
                rows = data.get('data', [])

                if fields and rows:
                    df = pd.DataFrame(rows, columns=fields)

                    # å„²å­˜ç‚º CSV æ ¼å¼åˆ° raw_data
                    df.to_csv(csv_path, index=False, encoding="utf-8-sig")

                    print(f"âœ… ETF è‚¡åˆ© JSONâ†’CSV è½‰æ›æˆåŠŸ: {len(df)} ç­†è³‡æ–™")
                    return True
                else:
                    print(f"âš ï¸ JSON è³‡æ–™æ ¼å¼ç•°å¸¸")
            else:
                print(f"âš ï¸ {year_str} ç„¡ ETF è‚¡åˆ©è³‡æ–™")
                print(f"API å›æ‡‰: {data}")
        else:
            print(f"âŒ JSON API è«‹æ±‚å¤±æ•—: {response.status_code}")

    except Exception as e:
        print(f"âŒ JSON ä¸‹è¼‰å¤±æ•—: {e}")

    return False


# =========================
# Helper: clean ETF dividend CSV
# =========================
def clean_etf_dividend_csv(path: str) -> pd.DataFrame:
    """æ¸…ç† ETF è‚¡åˆ© CSV æª”æ¡ˆ"""
    print(f"ğŸ§¹ æ¸…ç† ETF è‚¡åˆ©æª”æ¡ˆ: {os.path.basename(path)}")

    # å…ˆè®€å–æ–‡æœ¬æ‰¾åˆ°çœŸæ­£çš„è¡¨é ­ä½ç½®
    with open(path, "r", encoding="utf-8-sig", errors="ignore") as f:
        lines = f.readlines()

    header_idx = None
    for i, line in enumerate(lines):
        # å°‹æ‰¾åŒ…å« ETF ç›¸é—œæ¬„ä½çš„è¡¨é ­è¡Œ
        if any(keyword in line for keyword in ['ä»£è™Ÿ', 'è­‰åˆ¸ä»£è™Ÿ', 'ETF', 'åç¨±', 'è­‰åˆ¸ç°¡ç¨±', 'é™¤æ¯äº¤æ˜“æ—¥']):
            if line.count(',') > 2:  # ç¢ºä¿æ˜¯è¡¨æ ¼é–‹é ­
                header_idx = i
                break

    if header_idx is None:
        print(f"âš ï¸ ç„¡æ³•åœ¨ {os.path.basename(path)} æ‰¾åˆ°æœ‰æ•ˆçš„è¡¨é ­")
        # å˜—è©¦ç›´æ¥è®€å–
        try:
            df = pd.read_csv(path, encoding="utf-8-sig", dtype=str)
            if not df.empty:
                return df
        except:
            pass
        return pd.DataFrame()

    # ç”¨ pandas è¼‰å…¥ï¼Œè·³éå‰é¢çš„èªªæ˜è¡Œ
    try:
        df = pd.read_csv(path, encoding="utf-8-sig", dtype=str, engine="python",
                        on_bad_lines="skip", skiprows=header_idx)
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•è®€å– {os.path.basename(path)}: {e}")
        return pd.DataFrame()

    # æª¢æŸ¥æ˜¯å¦æœ‰è³‡æ–™
    if df.empty:
        print(f"âš ï¸ {os.path.basename(path)} ç‚ºç©ºæª”æ¡ˆ")
        return pd.DataFrame()

    # ç§»é™¤å…¨ç©ºåˆ—å’Œ Unnamed æ¬„ä½
    df = df.dropna(how="all")
    df = df.loc[:, ~df.columns.str.contains("^Unnamed")]

    print(f"âœ… {os.path.basename(path)} æ¸…ç†å®Œæˆï¼Œä¿ç•™ {len(df)} è¡Œ")

    return df


# =========================
# Helper: process ETF dividend data (èˆ‡dividendåŒæ­¥æ ¼å¼)
# =========================
def process_etf_dividend_data(df, year_str):
    """è™•ç† ETF è‚¡åˆ©è³‡æ–™ - èˆ‡dividendæ ¼å¼åŒæ­¥"""
    if df.empty:
        return df

    df_processed = df.copy()

    print(f"ğŸ”§ ETF è‚¡åˆ©è³‡æ–™è™•ç†ä¸­...")

    # 1. æ¬„ä½é‡æ–°å‘½å (èˆ‡dividendæ ¼å¼åŒæ­¥)
    if 'è­‰åˆ¸ä»£è™Ÿ' in df_processed.columns:
        df_processed = df_processed.rename(columns={'è­‰åˆ¸ä»£è™Ÿ': 'ä»£è™Ÿ'})
        print(f"   è­‰åˆ¸ä»£è™Ÿ â†’ ä»£è™Ÿ")

    if 'è­‰åˆ¸ç°¡ç¨±' in df_processed.columns:
        df_processed = df_processed.rename(columns={'è­‰åˆ¸ç°¡ç¨±': 'åç¨±'})
        print(f"   è­‰åˆ¸ç°¡ç¨± â†’ åç¨±")

    if 'æ”¶ç›Šåˆ†é…é‡‘é¡ (æ¯1å—ç›Šæ¬Šç›Šå–®ä½)' in df_processed.columns:
        df_processed = df_processed.rename(columns={'æ”¶ç›Šåˆ†é…é‡‘é¡ (æ¯1å—ç›Šæ¬Šç›Šå–®ä½)': 'é…æ¯'})
        print(f"   æ”¶ç›Šåˆ†é…é‡‘é¡ (æ¯1å—ç›Šæ¬Šç›Šå–®ä½) â†’ é…æ¯")

    # 2. å¹´åº¦è™•ç†ï¼šä¿æŒæ°‘åœ‹å¹´æ ¼å¼
    roc_year = int(year_str)
    df_processed['å¹´åº¦'] = roc_year  # ç›´æ¥ä½¿ç”¨æ°‘åœ‹å¹´
    print(f"   å¹´åº¦è¨­ç‚º: {roc_year} (æ°‘åœ‹å¹´)")

    # 3. å­£åˆ¥è™•ç†ï¼šä¾é™¤æ¯äº¤æ˜“æ—¥åˆ¤æ–·æœˆä»½ (åƒè€ƒdividendæ ¼å¼)
    if 'é™¤æ¯äº¤æ˜“æ—¥' in df_processed.columns:
        print(f"   æ­£åœ¨åˆ†æé™¤æ¯äº¤æ˜“æ—¥ä»¥åˆ¤æ–·æœˆä»½...")

        def determine_month_from_date(date_str):
            """å¾é™¤æ¯äº¤æ˜“æ—¥åˆ¤æ–·æœˆä»½ (åƒè€ƒdividendæ ¼å¼)"""
            if pd.isna(date_str) or date_str == '':
                return None

            date_str = str(date_str).strip()

            # å˜—è©¦æå–æœˆä»½
            # æ ¼å¼å¯èƒ½æ˜¯: 114å¹´01æœˆ22æ—¥, 2024/01/22, 01/22, ç­‰
            import re

            # åŒ¹é…å„ç¨®æ—¥æœŸæ ¼å¼ä¸­çš„æœˆä»½
            month_patterns = [
                r'(\d+)å¹´(\d+)æœˆ',  # 114å¹´01æœˆ22æ—¥
                r'(\d{4})[/-](\d{1,2})[/-]',  # 2024/01/22 æˆ– 2024-01-22
                r'(\d{1,2})[/-](\d{1,2})',  # 01/22
            ]

            month = None
            for pattern in month_patterns:
                match = re.search(pattern, date_str)
                if match:
                    if 'å¹´' in pattern:
                        month = int(match.group(2))  # æœˆä»½æ˜¯ç¬¬äºŒçµ„
                    else:
                        month = int(match.group(2)) if len(match.groups()) > 1 else int(match.group(1))
                    break

            if month is None:
                return "OTHER"

            # æ ¹æ“šæœˆä»½è¿”å›æ ¼å¼ (åƒè€ƒdividendçš„M{æœˆä»½}æ ¼å¼)
            if 1 <= month <= 12:
                return f"M{month:02d}"  # M01, M02, ..., M12
            else:
                return "OTHER"

        df_processed['å­£åˆ¥'] = df_processed['é™¤æ¯äº¤æ˜“æ—¥'].apply(determine_month_from_date)

        # çµ±è¨ˆæœˆä»½åˆ†å¸ƒ
        month_counts = df_processed['å­£åˆ¥'].value_counts()
        print(f"   æœˆä»½åˆ†å¸ƒ: {dict(month_counts)}")
    else:
        df_processed['å­£åˆ¥'] = "OTHER"
        print(f"   ç„¡é™¤æ¯äº¤æ˜“æ—¥æ¬„ä½ï¼Œå­£åˆ¥è¨­ç‚º OTHER")

    # 4. ç¢ºä¿é—œéµæ¬„ä½æ ¼å¼æ­£ç¢º
    if 'ä»£è™Ÿ' in df_processed.columns:
        df_processed['ä»£è™Ÿ'] = df_processed['ä»£è™Ÿ'].astype(str)

    if 'åç¨±' in df_processed.columns:
        df_processed['åç¨±'] = df_processed['åç¨±'].astype(str)

    # 5. æ•¸å€¼æ¬„ä½è½‰æ›
    numeric_columns = ['é…æ¯', 'å…¬å‘Šå¹´åº¦']
    existing_numeric_cols = [col for col in numeric_columns if col in df_processed.columns]

    if existing_numeric_cols:
        print(f"   è½‰æ›æ•¸å€¼æ¬„ä½: {existing_numeric_cols}")
        for col in existing_numeric_cols:
            # æ¸…ç†æ•¸å€¼ï¼šç§»é™¤é€—è™Ÿã€ç©ºæ ¼ã€ç‰¹æ®Šå­—ç¬¦
            df_processed[col] = df_processed[col].astype(str).str.replace(',', '')
            df_processed[col] = df_processed[col].str.replace(' ', '')
            df_processed[col] = df_processed[col].str.replace('--', '')
            df_processed[col] = df_processed[col].str.replace('-', '')
            df_processed[col] = df_processed[col].replace(['', 'nan', 'None', 'null'], None)

            # è½‰æ›ç‚ºæ•¸å€¼
            df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')

        print(f"   âœ… æˆåŠŸè½‰æ› {len(existing_numeric_cols)} å€‹æ•¸å€¼æ¬„ä½")

    # 6. é‡æ–°æ’åˆ—æ¬„ä½é †åº (èˆ‡dividendåŒæ­¥)
    cols = df_processed.columns.tolist()
    priority_cols = []

    for col_name in ['ä»£è™Ÿ', 'åç¨±', 'å¹´åº¦', 'å­£åˆ¥']:
        if col_name in cols:
            priority_cols.append(col_name)
            cols.remove(col_name)

    # é‡æ–°çµ„åˆæ¬„ä½é †åº
    new_cols = priority_cols + cols
    df_processed = df_processed[new_cols]

    print(f"âœ… ETF è‚¡åˆ©è³‡æ–™è™•ç†å®Œæˆ: {len(df_processed)} ç­†")
    print(f"   æœ€çµ‚æ¬„ä½é †åº: {new_cols[:6]}...")  # é¡¯ç¤ºå‰6å€‹æ¬„ä½

    return df_processed


# =========================
# Helper: log writer
# =========================
def write_log(year, report_name, csv_path, json_path, row_count):
    log_data = []
    if os.path.exists(log_path):
        try:
            with open(log_path, "r", encoding="utf-8") as f:
                log_data = json.load(f)
        except Exception:
            log_data = []

    entry = {
        "year": year,
        "report": report_name,
        "timestamp": datetime.now().isoformat(timespec="seconds"),
        "files": {
            "csv": csv_path if csv_path else None,
            "json": json_path if json_path else None
        },
        "total_rows": int(row_count)
    }

    log_data.append(entry)
    with open(log_path, "w", encoding="utf-8") as f:
        json.dump(log_data, f, ensure_ascii=False, indent=2)


# =========================
# Main Process
# =========================
for report_name, urls in report_types.items():
    if download_reports and 'all' not in download_reports and report_name not in download_reports:
        continue

    print(f"\n=== Start processing {report_name} ===")
    for year in range(start_year, end_year + 1):
        year_str = str(year)
        year_dir = os.path.join(base_dir, report_name, year_str)

        if only_merge:
            print(f"ğŸ”„ åƒ…åˆä½µæ¨¡å¼: è™•ç† {year_str} {report_name}")
            if not os.path.exists(year_dir):
                print(f"âŒ æ‰¾ä¸åˆ°è³‡æ–™å¤¾: {year_dir}")
                continue
        else:
            print(f"â¬‡ï¸ ä¸‹è¼‰æ¨¡å¼: è™•ç† {year_str} {report_name}")
            if os.path.exists(year_dir):
                shutil.rmtree(year_dir)
            os.makedirs(year_dir, exist_ok=True)

            # ETF è‚¡åˆ©è™•ç†
            if report_name == "etf_dividend":
                download_etf_dividend(year_str, year_dir)
            else:
                # ä¸€èˆ¬å ±è¡¨è™•ç†
                all_filenames = []

                # Step 1: æŠ“ CSV æª”å
                for market in markets:
                    for season in seasons:
                        ajax_url = (
                            urls["ajax"].format(year=year_str, market=market, season=season)
                            if report_name != "dividend"
                            else urls["ajax"].format(year=year_str, market=market)
                        )
                        try:
                            res = requests.get(ajax_url, headers=headers, verify=False, timeout=10)
                            res.encoding = "utf-8"
                            soup = BeautifulSoup(res.text, "lxml")
                            input_tags = soup.find_all("input", {"name": "filename"})
                            filenames = [tag.get("value") for tag in input_tags if tag.get("value")]
                            all_filenames.extend(filenames)
                        except Exception as e:
                            print(f"Fetch {year_str} {market} {season} filenames failed: {e}")
                        time.sleep(0.5)

                # å»é‡
                seen = set()
                unique_filenames = [f for f in all_filenames if not (f in seen or seen.add(f))]
                print(f"{year_str} found {len(all_filenames)} CSVs, {len(unique_filenames)} unique")

                # Step 2: ä¸‹è¼‰
                for fname in tqdm(unique_filenames, desc=f"{year_str} {report_name} download"):
                    save_path = os.path.join(year_dir, fname)
                    download_url = f"{urls['download_base']}?firstin=true&step=10&filename={fname}"
                    for attempt in range(3):
                        try:
                            r = requests.get(download_url, headers=headers, verify=False, timeout=10)
                            r.encoding = "big5"
                            with open(save_path, "w", encoding="utf-8-sig", newline="") as f:
                                f.write(r.text)
                            break
                        except Exception as e:
                            print(f"Download {fname} failed: {e} (try {attempt+1})")
                            time.sleep(2)
                    else:
                        print(f"âŒ {fname} download failed, skipped")

        # Step 3: æ¸…ç†èˆ‡åˆä½µ (ä¸‹è¼‰æ¨¡å¼å’Œåƒ…åˆä½µæ¨¡å¼éƒ½æœƒåŸ·è¡Œ)
        all_dfs = []
        csv_files = [f for f in os.listdir(year_dir) if f.endswith(".csv")]
        print(f"ğŸ“ æ‰¾åˆ° {len(csv_files)} å€‹ CSV æª”æ¡ˆ")

        for fname in csv_files:
            path = os.path.join(year_dir, fname)
            try:
                if report_name == "dividend":
                    df = clean_and_sort_dividend(path)
                elif report_name == "etf_dividend":
                    # ä½¿ç”¨å°ˆé–€çš„ ETF è‚¡åˆ©æ¸…ç†å‡½æ•¸
                    df = clean_etf_dividend_csv(path)
                    if not df.empty:
                        df = process_etf_dividend_data(df, year_str)
                else:
                    df = pd.read_csv(path, encoding="utf-8-sig", dtype=str)
                    df = df.dropna(how="all")

                # å…ˆä¸éæ¿¾æ¬„ä½ï¼Œä¿ç•™æ‰€æœ‰è³‡æ–™é€²è¡Œåˆä½µ
                if not df.empty:
                    all_dfs.append(df)
            except Exception as e:
                print(f"Read {fname} failed: {e}")

        # Step 4: åˆä½µå¾Œå†éæ¿¾æ¬„ä½å’Œæ’åº
        if all_dfs:
            # å…ˆåˆä½µæ‰€æœ‰è³‡æ–™
            combined_df = pd.concat(all_dfs, ignore_index=True)
            print(f"ğŸ“Š åˆä½µå®Œæˆï¼Œç¸½è¨ˆ {len(combined_df)} è¡Œï¼Œ{len(combined_df.columns)} æ¬„")

            # å…ˆæ•´ç†æ¬„ä½ï¼šçµ±ä¸€æ¬„ä½åç¨±å’Œæ ¼å¼
            combined_df = process_company_code_name(combined_df, report_name)

            # ç„¶å¾Œéæ¿¾æ¬„ä½ (ä½¿ç”¨çµ±ä¸€å¾Œçš„æ¬„ä½åç¨±)
            combined_df = filter_columns(combined_df, report_name)

            # ä¾ä»£è™Ÿæ’åº (ETF èˆ‡ dividend æ ¼å¼çµ±ä¸€)
            if report_name == "etf_dividend":
                if 'ä»£è™Ÿ' in combined_df.columns:
                    combined_df = combined_df.sort_values(by='ä»£è™Ÿ', ascending=True, ignore_index=True)
                    print(f"ğŸ”¢ {report_name} ä¾ 'ä»£è™Ÿ' æ’åº")
            else:
                combined_df = sort_by_company_code(combined_df, report_name)

            csv_path = json_path = None

            if "csv" in save_format:
                csv_path = os.path.join(csv_output_dir, f"{year_str}-{report_name}.csv")
                combined_df.to_csv(csv_path, index=False, encoding="utf-8-sig")
                print(f"âœ… CSV saved: {csv_path}")

            if "json" in save_format:
                json_path = os.path.join(json_output_dir, f"{year_str}-{report_name}.json")
                combined_df.to_json(json_path, orient="records", force_ascii=False, indent=2)
                print(f"âœ… JSON saved: {json_path}")

            write_log(year_str, report_name, csv_path, json_path, len(combined_df))
            print(f"ğŸ“ Log updated for {year_str} {report_name} - Total rows: {len(combined_df)}")
        else:
            print(f"âŒ {year_str} {report_name} no valid CSVs to merge")

print("\nğŸ‰ æ‰€æœ‰è™•ç†å®Œæˆï¼")