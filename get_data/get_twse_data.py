import requests
from bs4 import BeautifulSoup
import os
import shutil
import time
import certifi
from tqdm import tqdm
import pandas as pd
import json
from datetime import datetime

import urllib3
# å¿½ç•¥ SSL è­‰æ›¸è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

"""
TWSE è²¡å ±è³‡æ–™ä¸‹è¼‰èˆ‡åˆä½µå·¥å…·
==============================

æ–°å¢åŠŸèƒ½èªªæ˜ï¼š
1. åƒ…åˆä½µæ¨¡å¼ï¼šè¨­å®š only_merge = Trueï¼Œè·³éä¸‹è¼‰ç›´æ¥åˆä½µç¾æœ‰çš„ raw_data æª”æ¡ˆ
2. æ¬„ä½éæ¿¾ï¼šè¨­å®š keep_columns å­—å…¸ä¾†æŒ‡å®šæ¯ç¨®å ±è¡¨è¦ä¿ç•™çš„æ¬„ä½
3. è‡ªå‹•æ’åºï¼šåˆä½µå¾Œçš„è³‡æ–™æœƒè‡ªå‹•ä¾å…¬å¸ä»£è™Ÿæ’åº

è™•ç†æµç¨‹ï¼š
1. ä¸‹è¼‰æˆ–è®€å– CSV æª”æ¡ˆ
2. åˆä½µæ‰€æœ‰è³‡æ–™
3. éæ¿¾æŒ‡å®šæ¬„ä½ï¼ˆå¦‚æœ‰è¨­å®šï¼‰
4. ä¾å…¬å¸ä»£è™Ÿæ’åº
5. å„²å­˜ç‚º CSV/JSON æ ¼å¼

ä½¿ç”¨ç¯„ä¾‹ï¼š
---------
# ä¸€èˆ¬æ¨¡å¼ï¼ˆä¸‹è¼‰ + åˆä½µï¼‰
only_merge = False
keep_columns = {}

# åƒ…åˆä½µæ¨¡å¼ + æ¬„ä½éæ¿¾
only_merge = True
keep_columns = {
    'balance_sheet': ['å…¬å¸ä»£è™Ÿ', 'å…¬å¸åç¨±', 'æµå‹•è³‡ç”¢', 'è³‡ç”¢ç¸½é¡'],
    'income_statement': ['å…¬å¸ä»£è™Ÿ', 'å…¬å¸åç¨±', 'ç‡Ÿæ¥­æ”¶å…¥', 'ç¨…å¾Œæ·¨åˆ©'],
    'dividend': ['å…¬å¸ä»£è™Ÿåç¨±', 'è‚¡æ±æœƒæ—¥æœŸ', 'è‚¡åˆ©åˆè¨ˆ']
}
"""

# =========================
# Config
# =========================
start_year = 109
end_year = 114
markets = ["sii", "otc"]
seasons = ["01", "02", "03", "04"]

download_reports = ['dividend']  # or ['dividend', 'balance_sheet']
save_format = ['csv', 'json']  # å¯ç‚º ['csv'], ['json'], ['csv', 'json']

# æ–°å¢åŠŸèƒ½è¨­å®š
only_merge = True  # è¨­ç‚º True æ™‚åªåšåˆä½µï¼Œä¸ä¸‹è¼‰

# æŒ‡å®šè¦ä¿ç•™çš„æ¬„ä½ï¼Œæ ¼å¼: {'report_name': ['column1', 'column2', ...]}
# æ¬„ä½ä¿ç•™è¨­å®šç¯„ä¾‹ï¼š
# keep_columns = {
#     'balance_sheet': ['å…¬å¸ä»£è™Ÿ', 'å…¬å¸åç¨±', 'æµå‹•è³‡ç”¢', 'è³‡ç”¢ç¸½é¡'],
#     'income_statement': ['å…¬å¸ä»£è™Ÿ', 'å…¬å¸åç¨±', 'ç‡Ÿæ¥­æ”¶å…¥', 'ç¨…å¾Œæ·¨åˆ©'],
#     'dividend': ['å…¬å¸ä»£è™Ÿåç¨±', 'è‚¡æ±æœƒæ—¥æœŸ', 'è‚¡åˆ©åˆè¨ˆ'],
#     'cash_flow': ['å…¬å¸ä»£è™Ÿ', 'å…¬å¸åç¨±', 'ç‡Ÿæ¥­æ´»å‹•ä¹‹ç¾é‡‘æµé‡']
# }
keep_columns = {
    'balance_sheet': [
        # è­˜åˆ¥èˆ‡æ™‚é–“åºåˆ—
        'å…¬å¸ä»£è™Ÿ',
        'å…¬å¸åç¨±',
        'å¹´åº¦',
        'å­£åˆ¥',
        # æ ¸å¿ƒè¨ˆç®— (ROE, ç›ˆå†ç‡) - é€™äº›æ¬„ä½å·²ç¢ºèªå­˜åœ¨æ–¼ Source [1] ä¸­
        'æ­¸å±¬æ–¼æ¯å…¬å¸æ¥­ä¸»ä¹‹æ¬Šç›Šåˆè¨ˆ',  # ROE åˆ†æ¯
        'ä¸å‹•ç”¢åŠè¨­å‚™ï¼æ·¨é¡',       # ç›ˆå†ç‡çµ„ä»¶
        'ç„¡å½¢è³‡ç”¢ï¼æ·¨é¡',           # ç›ˆå†ç‡çµ„ä»¶
        # é¢¨éšªèˆ‡è¼”åŠ©è³‡è¨Š - é€™äº›æ¬„ä½å·²ç¢ºèªå­˜åœ¨æ–¼ Source [1] ä¸­
        'æµå‹•è³‡ç”¢',
        'è³‡ç”¢ç¸½é¡',
        'æµå‹•è² å‚µ',
        'éæµå‹•è² å‚µ',
        'éæ§åˆ¶æ¬Šç›Š',
        'æ¯è‚¡åƒè€ƒæ·¨å€¼',
    ],
    'income_statement': [
        # è­˜åˆ¥èˆ‡æ™‚é–“åºåˆ—
        'å…¬å¸ä»£è™Ÿ',
        'å…¬å¸åç¨±',
        'å¹´åº¦',
        'å­£åˆ¥',
        'å‡ºè¡¨æ—¥æœŸ',
        # æ ¸å¿ƒè¨ˆç®— (ROE, ç©©å®šæ€§)
        'æ·¨åˆ©ï¼ˆæï¼‰æ­¸å±¬æ–¼æ¯å…¬å¸æ¥­ä¸»',
        'ç‡Ÿæ¥­æ”¶å…¥',
        'ç‡Ÿæ¥­æˆæœ¬',
        # è¼”åŠ©èˆ‡ç›¸å®¹æ€§
        'ç¨…å¾Œæ·¨åˆ©',
        'åŸºæœ¬æ¯è‚¡ç›ˆé¤˜ï¼ˆå…ƒï¼‰',
    ],
    'dividend': [
        # è­˜åˆ¥èˆ‡æ™‚é–“åºåˆ—
        'å…¬å¸ä»£è™Ÿåç¨±',  # åŸå§‹æ¬„ä½ï¼Œæœƒè¢«æ‹†åˆ†æˆå…¬å¸ä»£è™Ÿå’Œå…¬å¸åç¨±
        'è‚¡æ±æœƒæ—¥æœŸ',
        'è‚¡åˆ©æ‰€å±¬å¹´(å­£)åº¦',  # åŸå§‹æ¬„ä½ï¼Œæœƒè¢«æ‹†åˆ†æˆå¹´åº¦å’Œå­£åˆ¥
        'è‚¡åˆ©æ‰€å±¬æœŸé–“',
        'æ±ºè­°ï¼ˆæ“¬è­°ï¼‰é€²åº¦',
        # æ ¸å¿ƒè¨ˆç®— (ç¾é‡‘é…ç™¼/IRR)
        'è‚¡æ±é…ç™¼-ç›ˆé¤˜åˆ†é…ä¹‹ç¾é‡‘è‚¡åˆ©(å…ƒ/è‚¡)',
        'è‚¡æ±é…ç™¼-æ³•å®šç›ˆé¤˜å…¬ç©ç™¼æ”¾ä¹‹ç¾é‡‘(å…ƒ/è‚¡)',
        'è‚¡æ±é…ç™¼-è³‡æœ¬å…¬ç©ç™¼æ”¾ä¹‹ç¾é‡‘(å…ƒ/è‚¡)',
        'è‚¡æ±é…ç™¼-è‚¡æ±é…ç™¼ä¹‹ç¾é‡‘(è‚¡åˆ©)ç¸½é‡‘é¡(å…ƒ)',
        # é…è‚¡ç›¸é—œ
        'è‚¡æ±é…ç™¼-ç›ˆé¤˜è½‰å¢è³‡é…è‚¡(å…ƒ/è‚¡)',
        'è‚¡æ±é…ç™¼-æ³•å®šç›ˆé¤˜å…¬ç©è½‰å¢è³‡é…è‚¡(å…ƒ/è‚¡)',
        'è‚¡æ±é…ç™¼-è³‡æœ¬å…¬ç©è½‰å¢è³‡é…è‚¡(å…ƒ/è‚¡)',
        'è‚¡æ±é…ç™¼-è‚¡æ±é…è‚¡ç¸½è‚¡æ•¸(è‚¡)'
    ],
    'cash_flow': [
        # è­˜åˆ¥èˆ‡æ™‚é–“åºåˆ—
        'å…¬å¸ä»£è™Ÿ',
        'å…¬å¸åç¨±',
        'å¹´åº¦',
        'å­£åˆ¥',
        # æ ¸å¿ƒè¨ˆç®— (é¢¨éšªé©—è­‰)
        'ç‡Ÿæ¥­æ´»å‹•ä¹‹æ·¨ç¾é‡‘æµå…¥ï¼ˆæµå‡ºï¼‰',
    ]
}

report_types = {
    "balance_sheet": {
        "ajax": "https://mopsov.twse.com.tw/mops/web/ajax_t163sb05?year={year}&TYPEK={market}&season={season}&firstin=1",
        "download_base": "https://mopsov.twse.com.tw/server-java/t105sb02"
    },
    "dividend": {
        "ajax": "https://mopsov.twse.com.tw/server-java/t05st09sub?YEAR={year}&qryType=2&TYPEK={market}&step=1",
        "download_base": "https://mopsov.twse.com.tw/server-java/t105sb02"
    },
    "income_statement": {
        "ajax": "https://mopsov.twse.com.tw/mops/web/ajax_t163sb04?year={year}&TYPEK={market}&season={season}&firstin=1",
        "download_base": "https://mopsov.twse.com.tw/server-java/t105sb02"
    },
    "cash_flow": {
        "ajax": "https://mopsov.twse.com.tw/mops/web/ajax_t163sb20?year={year}&TYPEK={market}&season={season}&firstin=1",
        "download_base": "https://mopsov.twse.com.tw/server-java/t105sb02"
    }
}

headers = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)"}

base_dir = "raw_data"
merge_dir = "merged_data"
log_path = os.path.join(merge_dir, "log.json")
os.makedirs(merge_dir, exist_ok=True)

# =========================
# Helper: sort by company code
# =========================
def sort_by_company_code(df: pd.DataFrame, report_name: str) -> pd.DataFrame:
    """ä¾å…¬å¸ä»£è™Ÿæ’åº"""
    if df.empty:
        return df

    # æ‰¾å‡ºå…¬å¸ä»£è™Ÿæ¬„ä½
    company_code_col = None
    if "å…¬å¸ä»£è™Ÿ" in df.columns:
        company_code_col = "å…¬å¸ä»£è™Ÿ"
    elif "å…¬å¸ä»£è™Ÿåç¨±" in df.columns:
        company_code_col = "å…¬å¸ä»£è™Ÿåç¨±"
    else:
        # å˜—è©¦æ‰¾åˆ°åŒ…å«"å…¬å¸ä»£è™Ÿ"çš„æ¬„ä½
        for col in df.columns:
            if "å…¬å¸ä»£è™Ÿ" in col:
                company_code_col = col
                break

    if company_code_col is None:
        print(f"âš ï¸ {report_name} æ‰¾ä¸åˆ°å…¬å¸ä»£è™Ÿæ¬„ä½ï¼Œè·³éæ’åº")
        return df

    print(f"ğŸ”¢ {report_name} ä¾ '{company_code_col}' æ’åº")

    # å¦‚æœæ˜¯å…¬å¸ä»£è™Ÿåç¨±æ ¼å¼ (ä¾‹å¦‚: "2330 - å°ç©é›»")ï¼Œæå–å‰é¢çš„æ•¸å­—é€²è¡Œæ’åº
    if company_code_col == "å…¬å¸ä»£è™Ÿåç¨±" or (len(df) > 0 and " - " in str(df[company_code_col].iloc[0])):
        # å‰µå»ºä¸€å€‹è‡¨æ™‚æ¬„ä½ç”¨æ–¼æ’åº
        df_sorted = df.copy()
        df_sorted['_sort_key'] = df_sorted[company_code_col].astype(str).str.extract(r'(\d+)')[0]
        df_sorted['_sort_key'] = pd.to_numeric(df_sorted['_sort_key'], errors='coerce')
        df_sorted = df_sorted.sort_values(by='_sort_key', ascending=True, ignore_index=True)
        df_sorted = df_sorted.drop(columns=['_sort_key'])
        return df_sorted
    else:
        # ç›´æ¥ä¾å…¬å¸ä»£è™Ÿæ’åºï¼ˆé©ç”¨æ–¼å·²æ‹†åˆ†çš„å…¬å¸ä»£è™Ÿæ¬„ä½ï¼‰
        # ä¿æŒå…¬å¸ä»£è™Ÿç‚ºå­—ä¸²æ ¼å¼ï¼Œä½†ç”¨æ•¸å­—æ’åº
        df_sorted = df.copy()

        # å‰µå»ºè‡¨æ™‚æ’åºéµï¼Œæå–å…¬å¸ä»£è™Ÿä¸­çš„æ•¸å­—éƒ¨åˆ†
        df_sorted['_sort_key'] = df_sorted[company_code_col].astype(str).str.extract(r'(\d+)')[0]
        df_sorted['_sort_key'] = pd.to_numeric(df_sorted['_sort_key'], errors='coerce')

        # æŒ‰æ•¸å­—æ’åºä½†ä¿æŒåŸå§‹å­—ä¸²æ ¼å¼
        df_sorted = df_sorted.sort_values(by='_sort_key', ascending=True, ignore_index=True)
        df_sorted = df_sorted.drop(columns=['_sort_key'])

        return df_sorted


# =========================
# Helper: process company code name
# =========================
def process_company_code_name(df: pd.DataFrame, report_name: str) -> pd.DataFrame:
    """
    è™•ç†è‚¡åˆ©è³‡æ–™è¡¨çš„æ¬„ä½æ‹†åˆ†ï¼š
    1. å…¬å¸ä»£è™Ÿåç¨± â†’ å…¬å¸ä»£è™Ÿ + å…¬å¸åç¨±
    2. è‚¡åˆ©æ‰€å±¬å¹´(å­£)åº¦ â†’ å¹´åº¦(æ•´æ•¸) + å­£åˆ¥(Q1, Q2, Q3, Q4, H1, H2, Y1)
    
    Args:
        df: è³‡æ–™æ¡†
        report_name: å ±è¡¨åç¨±
        
    Returns:
        è™•ç†å¾Œçš„è³‡æ–™æ¡†
    """
    if df.empty:
        return df
    
    # åªè™•ç†è‚¡åˆ©è³‡æ–™è¡¨
    if report_name == "dividend":
        df_processed = df.copy()
        
        # 1. æ‹†åˆ†å…¬å¸ä»£è™Ÿåç¨±æ¬„ä½
        if "å…¬å¸ä»£è™Ÿåç¨±" in df_processed.columns:
            print(f"ğŸ”§ {report_name} æ­£åœ¨æ‹†åˆ†å…¬å¸ä»£è™Ÿåç¨±æ¬„ä½...")
            
            # æ‹†åˆ†å…¬å¸ä»£è™Ÿåç¨± (æ ¼å¼: "1234 - å…¬å¸åç¨±")
            company_info = df_processed["å…¬å¸ä»£è™Ÿåç¨±"].str.split(" - ", n=1, expand=True)
            
            # æ–°å¢å…¬å¸ä»£è™Ÿå’Œå…¬å¸åç¨±æ¬„ä½
            df_processed["å…¬å¸ä»£è™Ÿ"] = company_info[0].str.strip()
            df_processed["å…¬å¸åç¨±"] = company_info[1].str.strip()
            
            # ç§»é™¤åŸå§‹çš„å…¬å¸ä»£è™Ÿåç¨±æ¬„ä½
            df_processed = df_processed.drop(columns=["å…¬å¸ä»£è™Ÿåç¨±"])
            
            print(f"âœ… æˆåŠŸæ‹†åˆ†å…¬å¸ä»£è™Ÿåç¨±æ¬„ä½")
        
        # 2. æ‹†åˆ†è‚¡åˆ©æ‰€å±¬å¹´(å­£)åº¦æ¬„ä½
        if "è‚¡åˆ©æ‰€å±¬å¹´(å­£)åº¦" in df_processed.columns:
            print(f"ğŸ”§ {report_name} æ­£åœ¨æ‹†åˆ†è‚¡åˆ©æ‰€å±¬å¹´(å­£)åº¦æ¬„ä½...")
            
            # æå–å¹´åº¦ (ä¾‹å¦‚: "113å¹´ å¹´åº¦" â†’ 113)
            df_processed["å¹´åº¦"] = df_processed["è‚¡åˆ©æ‰€å±¬å¹´(å­£)åº¦"].str.extract(r'(\d+)å¹´')[0]
            df_processed["å¹´åº¦"] = pd.to_numeric(df_processed["å¹´åº¦"], errors='coerce').astype('Int64')
            
            # æå–å­£åˆ¥ä¸¦æ¨™æº–åŒ–
            def standardize_period(period_str):
                if pd.isna(period_str):
                    return None
                
                period_str = str(period_str).strip()
                
                # å¹´åº¦
                if "å¹´åº¦" in period_str:
                    return "YEAR"
                # å­£åº¦
                elif "ç¬¬1å­£" in period_str:
                    return "Q1"
                elif "ç¬¬2å­£" in period_str:
                    return "Q2"
                elif "ç¬¬3å­£" in period_str:
                    return "Q3"
                elif "ç¬¬4å­£" in period_str:
                    return "Q4"
                # åŠå¹´
                elif "ä¸ŠåŠå¹´" in period_str:
                    return "H1"
                elif "ä¸‹åŠå¹´" in period_str:
                    return "H2"
                # æœˆä»½ (å¦‚æœæœ‰çš„è©±)
                elif "æœˆ" in period_str:
                    month_match = pd.Series([period_str]).str.extract(r'ç¬¬?(\d+)æœˆ')[0].iloc[0]
                    if month_match:
                        return f"M{month_match.zfill(2)}"
                
                return "OTHER"
            
            df_processed["å­£åˆ¥"] = df_processed["è‚¡åˆ©æ‰€å±¬å¹´(å­£)åº¦"].apply(standardize_period)
            
            # ç§»é™¤åŸå§‹çš„è‚¡åˆ©æ‰€å±¬å¹´(å­£)åº¦æ¬„ä½
            df_processed = df_processed.drop(columns=["è‚¡åˆ©æ‰€å±¬å¹´(å­£)åº¦"])
            
            print(f"âœ… æˆåŠŸæ‹†åˆ†è‚¡åˆ©æ‰€å±¬å¹´(å­£)åº¦æ¬„ä½")
        
        # 3. é‡æ–°æ’åˆ—æ¬„ä½é †åº
        cols = df_processed.columns.tolist()
        
        # ç¢ºå®šæ–°æ¬„ä½çš„é †åºï¼šå…¬å¸ä»£è™Ÿã€å…¬å¸åç¨±ã€å¹´åº¦ã€å­£åˆ¥
        priority_cols = []
        if "å…¬å¸ä»£è™Ÿ" in cols:
            priority_cols.append("å…¬å¸ä»£è™Ÿ")
            cols.remove("å…¬å¸ä»£è™Ÿ")
        if "å…¬å¸åç¨±" in cols:
            priority_cols.append("å…¬å¸åç¨±")
            cols.remove("å…¬å¸åç¨±")
        if "å¹´åº¦" in cols:
            priority_cols.append("å¹´åº¦")
            cols.remove("å¹´åº¦")
        if "å­£åˆ¥" in cols:
            priority_cols.append("å­£åˆ¥")
            cols.remove("å­£åˆ¥")
        
        # é‡æ–°çµ„åˆæ¬„ä½é †åº
        new_cols = priority_cols + cols
        df_processed = df_processed[new_cols]
        
        print(f"âœ… {report_name} æ¬„ä½è™•ç†å®Œæˆ")
        print(f"   æ–°å¢æ¬„ä½: {', '.join(priority_cols)}")
        
        return df_processed
    else:
        # å…¶ä»–å ±è¡¨ç›´æ¥è¿”å›åŸè³‡æ–™æ¡†
        return df
# =========================
# Helper: filter columns
# =========================
def filter_columns(df: pd.DataFrame, report_name: str) -> pd.DataFrame:
    """æ ¹æ“šè¨­å®šéæ¿¾æ¬„ä½"""
    if not keep_columns or report_name not in keep_columns:
        print(f"ğŸ“‹ {report_name} æœªè¨­å®šæ¬„ä½éæ¿¾ï¼Œä¿ç•™æ‰€æœ‰ {len(df.columns)} æ¬„")
        return df

    columns_to_keep = keep_columns[report_name]
    existing_columns = [col for col in columns_to_keep if col in df.columns]

    if existing_columns:
        missing_columns = set(columns_to_keep) - set(existing_columns)
        if missing_columns:
            print(f"âš ï¸ {report_name} æ‰¾ä¸åˆ°æ¬„ä½: {list(missing_columns)}")

        print(f"ğŸ“‹ {report_name} æ¬„ä½éæ¿¾: {len(df.columns)} â†’ {len(existing_columns)} æ¬„")
        print(f"   ä¿ç•™æ¬„ä½: {existing_columns}")
        return df[existing_columns].copy()
    else:
        print(f"âš ï¸ {report_name} æ‰¾ä¸åˆ°ä»»ä½•æŒ‡å®šçš„æ¬„ä½ï¼Œä¿ç•™æ‰€æœ‰ {len(df.columns)} æ¬„")
        return df


# =========================
# Helper: clean + sort dividend CSV
# =========================
def clean_and_sort_dividend(path: str) -> pd.DataFrame:
    """å¼·åŒ–ç‰ˆè‚¡åˆ©å ±è¡¨æ¸…ç†ï¼šè·³éå‰ç½®èªªæ˜è¡Œï¼Œè¼‰å…¥å¾Œæ’åºä¸¦ç§»é™¤æœ‰å•é¡Œçš„åˆ—"""

    # å…ˆè®€å–æ–‡æœ¬æ‰¾åˆ°çœŸæ­£çš„è¡¨é ­ä½ç½®
    with open(path, "r", encoding="utf-8-sig", errors="ignore") as f:
        lines = f.readlines()

    header_idx = None
    for i, line in enumerate(lines):
        # å°‹æ‰¾åŒ…å« "å…¬å¸ä»£è™Ÿåç¨±" æˆ–åŒæ™‚åŒ…å« "å…¬å¸ä»£è™Ÿ" å’Œ "å…¬å¸åç¨±" çš„è¡¨é ­è¡Œ
        if ("å…¬å¸ä»£è™Ÿåç¨±" in line) or (("å…¬å¸ä»£è™Ÿ" in line) and ("å…¬å¸åç¨±" in line)):
            if line.count(",") > 2:  # ç¢ºä¿æ˜¯è¡¨æ ¼é–‹é ­
                header_idx = i
                break

    if header_idx is None:
        print(f"âš ï¸ ç„¡æ³•åœ¨ {os.path.basename(path)} æ‰¾åˆ°å…¬å¸ä»£è™Ÿæ¬„ä½")
        return pd.DataFrame()

    # ç”¨ pandas è¼‰å…¥ï¼Œè·³éå‰é¢çš„èªªæ˜è¡Œ
    try:
        df = pd.read_csv(path, encoding="utf-8-sig", dtype=str, engine="python",
                        on_bad_lines="skip", skiprows=header_idx)
    except:
        print(f"âš ï¸ ç„¡æ³•è®€å– {os.path.basename(path)}")
        return pd.DataFrame()

    # æª¢æŸ¥æ˜¯å¦æœ‰è³‡æ–™
    if df.empty:
        print(f"âš ï¸ {os.path.basename(path)} ç‚ºç©ºæª”æ¡ˆ")
        return pd.DataFrame()

    # ç¢ºå®šç¬¬ä¸€æ¬„çš„åç¨±ï¼ˆå¯èƒ½æ˜¯ "å…¬å¸ä»£è™Ÿåç¨±" æˆ– "å…¬å¸ä»£è™Ÿ"ï¼‰
    first_col = df.columns[0]
    if "å…¬å¸ä»£è™Ÿ" not in first_col:
        print(f"âš ï¸ ç„¡æ³•åœ¨ {os.path.basename(path)} æ‰¾åˆ°å…¬å¸ä»£è™Ÿæ¬„ä½")
        return pd.DataFrame()

    # ç§»é™¤å…¨ç©ºåˆ—
    df = df.dropna(how="all")

    # ç§»é™¤ Unnamed æ¬„ä½
    df = df.loc[:, ~df.columns.str.contains("^Unnamed")]

    # æŒ‰ç¬¬ä¸€æ¬„æ’åºï¼ˆå°‡æœ‰å•é¡Œçš„åˆ—æ’åˆ°ä¸€èµ·ï¼‰
    df = df.sort_values(by=first_col, ascending=True, ignore_index=True, na_position='last')

    # ç§»é™¤æœ‰å•é¡Œçš„åˆ—ï¼š
    # 1. ç¬¬ä¸€æ¬„ä¸åŒ…å« " - " çš„åˆ—ï¼ˆé™¤äº†è¡¨é ­ï¼‰
    # 2. ç¬¬ä¸€æ¬„åŒ…å«è¡¨é ­æ–‡å­—çš„é‡è¤‡åˆ—
    # 3. ç¬¬ä¸€æ¬„ç‚ºç©ºæˆ–åªæœ‰å°‘é‡æ–‡å­—çš„åˆ—

    mask_to_keep = pd.Series([True] * len(df))

    for i, val in enumerate(df[first_col]):
        val_str = str(val).strip()

        # è·³éç©ºå€¼
        if val_str in ['nan', '', 'None']:
            mask_to_keep[i] = False
            continue

        # ç§»é™¤é‡è¤‡çš„è¡¨é ­è¡Œ
        if "å…¬å¸ä»£è™Ÿ" in val_str and not " - " in val_str:
            mask_to_keep[i] = False
            continue

        # ç§»é™¤ä¸åŒ…å« " - " çš„è¡Œï¼ˆæ­£å¸¸çš„å…¬å¸ä»£è™Ÿæ‡‰è©²æ˜¯ "1234 - å…¬å¸åç¨±" æ ¼å¼ï¼‰
        if " - " not in val_str:
            mask_to_keep[i] = False
            continue

        # ç§»é™¤å¤ªçŸ­çš„è¡Œï¼ˆå¯èƒ½æ˜¯æ–·è¡Œé€ æˆçš„ï¼‰
        if len(val_str) < 5:
            mask_to_keep[i] = False
            continue

    # å¥—ç”¨éæ¿¾
    df_cleaned = df[mask_to_keep].copy()

    # é‡æ–°ç´¢å¼•
    df_cleaned.reset_index(drop=True, inplace=True)

    # æœ€å¾ŒæŒ‰æ­£å¸¸æ¬„ä½é‡æ–°æ’åº
    sort_cols = []
    if "å…¬å¸ä»£è™Ÿ" in df_cleaned.columns:
        sort_cols = [col for col in ["å…¬å¸ä»£è™Ÿ", "å…¬å¸åç¨±", "è‚¡æ±æœƒæ—¥æœŸ"] if col in df_cleaned.columns]
    elif "å…¬å¸ä»£è™Ÿåç¨±" in df_cleaned.columns:
        sort_cols = [col for col in ["å…¬å¸ä»£è™Ÿåç¨±", "è‚¡æ±æœƒæ—¥æœŸ"] if col in df_cleaned.columns]

    if sort_cols:
        df_cleaned = df_cleaned.sort_values(by=sort_cols, ascending=True, ignore_index=True)

    print(f"âœ… {os.path.basename(path)} æ¸…ç†å®Œæˆï¼Œä¿ç•™ {len(df_cleaned)} è¡Œ")

    return df_cleaned



# =========================
# Helper: log writer
# =========================
def write_log(year, report_name, csv_path, json_path, row_count):
    log_data = []
    if os.path.exists(log_path):
        try:
            with open(log_path, "r", encoding="utf-8") as f:
                log_data = json.load(f)
        except Exception:
            log_data = []

    entry = {
        "year": year,
        "report": report_name,
        "timestamp": datetime.now().isoformat(timespec="seconds"),
        "files": {
            "csv": csv_path if csv_path else None,
            "json": json_path if json_path else None
        },
        "total_rows": int(row_count)
    }

    log_data.append(entry)
    with open(log_path, "w", encoding="utf-8") as f:
        json.dump(log_data, f, ensure_ascii=False, indent=2)


# =========================
# Main Process
# =========================
for report_name, urls in report_types.items():
    if download_reports and 'all' not in download_reports and report_name not in download_reports:
        continue

    print(f"\n=== Start processing {report_name} ===")
    for year in range(start_year, end_year + 1):
        year_str = str(year)
        year_dir = os.path.join(base_dir, report_name, year_str)

        if only_merge:
            print(f"ğŸ”„ åƒ…åˆä½µæ¨¡å¼: è™•ç† {year_str} {report_name}")
            if not os.path.exists(year_dir):
                print(f"âŒ æ‰¾ä¸åˆ°è³‡æ–™å¤¾: {year_dir}")
                continue
        else:
            print(f"â¬‡ï¸ ä¸‹è¼‰æ¨¡å¼: è™•ç† {year_str} {report_name}")
            if os.path.exists(year_dir):
                shutil.rmtree(year_dir)
            os.makedirs(year_dir, exist_ok=True)

            all_filenames = []

            # Step 1: æŠ“ CSV æª”å
            for market in markets:
                for season in seasons:
                    ajax_url = (
                        urls["ajax"].format(year=year_str, market=market, season=season)
                        if report_name != "dividend"
                        else urls["ajax"].format(year=year_str, market=market)
                    )
                    try:
                        res = requests.get(ajax_url, headers=headers, verify=False, timeout=10)
                        res.encoding = "utf-8"
                        soup = BeautifulSoup(res.text, "lxml")
                        input_tags = soup.find_all("input", {"name": "filename"})
                        filenames = [tag.get("value") for tag in input_tags if tag.get("value")]
                        all_filenames.extend(filenames)
                    except Exception as e:
                        print(f"Fetch {year_str} {market} {season} filenames failed: {e}")
                    time.sleep(0.5)

            # å»é‡
            seen = set()
            unique_filenames = [f for f in all_filenames if not (f in seen or seen.add(f))]
            print(f"{year_str} found {len(all_filenames)} CSVs, {len(unique_filenames)} unique")

            # Step 2: ä¸‹è¼‰
            for fname in tqdm(unique_filenames, desc=f"{year_str} {report_name} download"):
                save_path = os.path.join(year_dir, fname)
                download_url = f"{urls['download_base']}?firstin=true&step=10&filename={fname}"
                for attempt in range(3):
                    try:
                        r = requests.get(download_url, headers=headers, verify=False, timeout=10)
                        r.encoding = "big5"
                        with open(save_path, "w", encoding="utf-8-sig", newline="") as f:
                            f.write(r.text)
                        break
                    except Exception as e:
                        print(f"Download {fname} failed: {e} (try {attempt+1})")
                        time.sleep(2)
                else:
                    print(f"âŒ {fname} download failed, skipped")

        # Step 3: æ¸…ç†èˆ‡åˆä½µ (ä¸‹è¼‰æ¨¡å¼å’Œåƒ…åˆä½µæ¨¡å¼éƒ½æœƒåŸ·è¡Œ)
        all_dfs = []
        csv_files = [f for f in os.listdir(year_dir) if f.endswith(".csv")]
        print(f"ğŸ“ æ‰¾åˆ° {len(csv_files)} å€‹ CSV æª”æ¡ˆ")

        for fname in csv_files:
            path = os.path.join(year_dir, fname)
            try:
                if report_name == "dividend":
                    df = clean_and_sort_dividend(path)
                else:
                    df = pd.read_csv(path, encoding="utf-8-sig", dtype=str)
                    df = df.dropna(how="all")

                # å…ˆä¸éæ¿¾æ¬„ä½ï¼Œä¿ç•™æ‰€æœ‰è³‡æ–™é€²è¡Œåˆä½µ
                all_dfs.append(df)
            except Exception as e:
                print(f"Read {fname} failed: {e}")

        # Step 4: åˆä½µå¾Œå†éæ¿¾æ¬„ä½å’Œæ’åº
        if all_dfs:
            # å…ˆåˆä½µæ‰€æœ‰è³‡æ–™
            combined_df = pd.concat(all_dfs, ignore_index=True)
            print(f"ğŸ“Š åˆä½µå®Œæˆï¼Œç¸½è¨ˆ {len(combined_df)} è¡Œï¼Œ{len(combined_df.columns)} æ¬„")

            # åˆä½µå¾Œå†éæ¿¾æ¬„ä½
            combined_df = filter_columns(combined_df, report_name)

            # æ•´ç†æ¬„ä½ï¼šå°‡è‚¡åˆ©è³‡æ–™è¡¨çš„"å…¬å¸ä»£è™Ÿåç¨±"åˆ†æˆ"å…¬å¸ä»£è™Ÿ"å’Œ"å…¬å¸åç¨±"å…©æ¬„
            combined_df = process_company_code_name(combined_df, report_name)

            # ä¾å…¬å¸ä»£è™Ÿæ’åº
            combined_df = sort_by_company_code(combined_df, report_name)

            csv_path = json_path = None

            if "csv" in save_format:
                csv_path = os.path.join(merge_dir, f"{year_str}-{report_name}.csv")
                combined_df.to_csv(csv_path, index=False, encoding="utf-8-sig")
                print(f"âœ… CSV saved: {csv_path}")

            if "json" in save_format:
                json_path = os.path.join(merge_dir, f"{year_str}-{report_name}.json")
                combined_df.to_json(json_path, orient="records", force_ascii=False, indent=2)
                print(f"âœ… JSON saved: {json_path}")

            write_log(year_str, report_name, csv_path, json_path, len(combined_df))
            print(f"ğŸ“ Log updated for {year_str} {report_name} - Total rows: {len(combined_df)}")
        else:
            print(f"âŒ {year_str} {report_name} no valid CSVs to merge")
